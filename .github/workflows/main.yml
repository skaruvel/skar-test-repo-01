- name: Run Databricks Notebook
  # You may pin to the exact commit or the version.
  # uses: databricks/run-notebook@f381bd756ae171a8f5a55b8dc5d5d760ef914daf
  uses: databricks/run-notebook@v0.0.3
  with:
    # Note: either local-notebook-path or workspace-notebook-path must be specified. Relative path to the notebook in the current Git repo, e.g. "path/to/my_notebook.py". If specified, then the specified notebook at local path will be uploaded to a temporary workspace directory under workspace-temp-dir and executed as a  one-time job.
Known limitation: When used with `git-commit`, `git-branch` or `git-tag`, the supplied `local-notebook-path`  must be a Databricks notebook exported as source via the UI (https://docs.databricks.com/notebooks/notebooks-manage.html#export-a-notebook) or REST API (https://docs.databricks.com/dev-tools/api/latest/workspace.html#export), rather than an arbitrary Python/Scala/R file.

    local-notebook-path: # optional
    # Note: either local-notebook-path or workspace-notebook-path must be specified. Absolute path in the Databricks workspace of an existing notebook to run, e.g.
 "/Users/john.doe@databricks.com/My Notebook", "/Repos/my-repo-name/notebooks/My Notebook".

    workspace-notebook-path: # optional
    # Hostname of the Databricks workspace in which to run the notebook. If unspecified, the hostname will be inferred from the DATABRICKS_HOST environment variable. Either this parameter or the DATABRICKS_HOST environment variable must be set.

    databricks-host: # optional
    # Databricks REST API token to use to run the notebook. If unspecified, the token will be inferred from the DATABRICKS_TOKEN environment variable. Either this parameter or the DATABRICKS_TOKEN environment variable must be set.

    databricks-token: # optional
    # Optional base directory in the workspace workspace under which to upload the local-notebook-path. If specified, the Action will create a random subdirectory under the specified directory, and upload the notebook immediately under the subdirectory. For example, if /tmp/actions is specified along with a notebook path of machine-learning/notebooks/my-notebook.py, the Action will upload to tmp/actions/<uuid>/my-notebook.py.

    workspace-temp-dir: # optional, default is /tmp/databricks-github-actions
    # Note: either existing-cluster-id or new-cluster-json must be specified.
JSON string describing the cluster on which to which execute the notebook. A new cluster with the specified attributes will be launched to run the notebook. For example, on Azure Databricks: {
    "num_workers": 1,
    "spark_version": "10.4.x-scala2.12",
    "node_type_id": "Standard_D3_v2"
}
On AWS:
{
    "num_workers": 1,
    "spark_version": "10.4.x-scala2.12",
    "node_type_id": "i3.xlarge"
}
On GCP:
{
    "num_workers": 1,
    "spark_version": "10.4.x-scala2.12",
    "node_type_id": "n1-highmem-4"
}

See docs for the "new_cluster" field in https://docs.databricks.com/dev-tools/api/latest/jobs.html#operation/JobsRunsSubmit for more details on how to specify this input.

    new-cluster-json: # optional
    # Note: either existing-cluster-id or new-cluster-json must be specified.
The string ID of an existing cluster on which to execute the notebook. We recommend specifying new-cluster-json instead for greater reliability. See docs for the "existing_cluster_id" field in https://docs.databricks.com/dev-tools/api/latest/jobs.html#operation/JobsRunsSubmit for details.

    existing-cluster-id: # optional
    # JSON string containing a list of libraries (e.g. [{"pypi": "sklearn"}, {"pypi": "mlflow"}]) to be installed on the cluster that will execute the notebook as a job.
See docs at https://docs.databricks.com/dev-tools/api/latest/libraries.html#library for details on how to specify this input.

    libraries-json: # optional
    # JSON string containing a list of parameters (e.g. [{"key": "...", "value": "..."}] passed to the notebook run. See docs for the "base_parameters" field in https://docs.databricks.com/dev-tools/api/latest/jobs.html#operation/JobsRunsSubmit for details on how to specify this input.

    notebook-params-json: # optional
    # JSON string containing a list of permissions to set on the job. The list must have elements of the form: email address for the user -> permission or group name -> permmision. e.g. [
  {"user_name": "userName@example.com", "permission_level": "CAN_MANAGE"},
  {"group_name": "users", "permission_level": "CAN_VIEW"}
]
Please refer to access_control_list at https://docs.databricks.com/dev-tools/api/latest/jobs.html#operation/JobsRunsSubmit for details on how to configure this input.

    access-control-list-json: # optional
    # Timeout, in seconds, for the one-time Databricks notebook job run. If not specified, the Databricks job will continue to run even if the current GitHub Workflow times out

    timeout-seconds: # optional
    # An optional name for the one-time notebook job run. Please refer to run_name at
      https://docs.databricks.com/dev-tools/api/latest/jobs.html#operation/JobsRunsSubmit
      for details on how to configure this input.

    run-name: # optional
    # If specified, the notebook is run on Databricks within a temporary checkout of the entire git repo at the specified git commit. The local-notebook-path input must be specified when using this parameter, and you must configure Git credentials (see https://docs.databricks.com/repos/index.html #configure-your-git-integration-with-databricks). This functionality can be useful for running a notebook that depends on other files within the same repo (e.g imports Python modules within the same repo) or that was developed using [Databricks Repos] (https://docs.databricks.com/repos/index.html). For example, you can set `git-commit` to `GITHUB_SHA` to run the notebook in the context of its enclosing repo against the commit that triggered the current GitHub Workflow, e.g. to run a notebook from the current PR branch.
Known limitation: The supplied `local-notebook-path` must be a Databricks notebook exported as source via the UI (https://docs.databricks.com/notebooks/notebooks-manage.html#export-a-notebook) or REST API (https://docs.databricks.com/dev-tools/api/latest/workspace.html#export), rather than an arbitrary Python/Scala/R file.

    git-commit: # optional
    # If specified, the notebook is run on Databricks within a temporary checkout of the entire git repo at the specified git branch. The local-notebook-path input must be specified when using this parameter, and you must configure Git credentials (see https://docs.databricks.com/repos/index.html #configure-your-git-integration-with-databricks). This functionality can be useful for running a notebook that depends on other files within the same repo (e.g imports Python modules within the same repo) or that was developed using [Databricks Repos] (https://docs.databricks.com/repos/index.html).
Known limitation: The supplied `local-notebook-path` must be a Databricks notebook exported as source via the UI (https://docs.databricks.com/notebooks/notebooks-manage.html#export-a-notebook) or REST API (https://docs.databricks.com/dev-tools/api/latest/workspace.html#export), rather than an arbitrary Python/Scala/R file.

    git-branch: # optional
    # If specified, the notebook is run on Databricks within a temporary checkout of the entire git repo at the specified git tag. The local-notebook-path input must be specified when using this parameter, and you must configure Git credentials (see https://docs.databricks.com/repos/index.html #configure-your-git-integration-with-databricks). This functionality can be useful for running a notebook that depends on other files within the same repo (e.g imports Python modules within the same repo) or that was developed using [Databricks Repos] (https://docs.databricks.com/repos/index.html).
Known limitation: The supplied `local-notebook-path` must be a Databricks notebook exported as source via the UI (https://docs.databricks.com/notebooks/notebooks-manage.html#export-a-notebook) or REST API (https://docs.databricks.com/dev-tools/api/latest/workspace.html#export), rather than an arbitrary Python/Scala/R file.

    git-tag: # optional
    # If specified, the notebook is run on Databricks within a temporary checkout of the entire git repo using the specified git provider. The local-notebook-path input as well as one of git-tag, git-commit, or git-branch must be specified when using this parameter, and you must configure Git credentials (see https://docs.databricks.com/repos/index.html #configure-your-git-integration-with-databricks). The default value for this input is 'gitHub', but if the repository is hosted in Github enterprise then one can specify 'gitHubEnterprise'.

    git-provider: # optional, default is gitHub
    # If specified, this action will post a PR comment containing the notebook output on run completion, using the specified GitHub token to post the comment. Note that no comment will be posted if this action is used in a workflow triggered from a source other than a pull request (e.g. a manually triggered workflow) The comment will consist of the output of the notebook. If the token is not provided then there will be no attempt to comment on the PR.
For example, you can set the value of this input to 'secrets.GITHUB_TOKEN', which by default has the necessary permissions to comment on PRs (see 'Default access (permissive)' in https://docs.github.com/en/actions/security-guides/ automatic-token-authentication#permissions-for-the-github_token)

    pr-comment-github-token: # optional
